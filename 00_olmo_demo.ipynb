{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff37fa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home04/cfang/.conda/envs/axolotl/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from config import storage_dir, hf_cache_dir\n",
    "from datasets import load_dataset\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e147ad11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "olmo = AutoModelForCausalLM.from_pretrained(\n",
    "    \"allenai/OLMo-2-1124-7B\", cache_dir=hf_cache_dir,\n",
    "    device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-2-1124-7B\")\n",
    "message = [\"Language modeling is \"]\n",
    "\n",
    "inputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)\n",
    "response = olmo.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1949187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 209/209 [09:53<00:00,  2.84s/files]\n",
      "Downloading data: 100%|██████████| 209/209 [09:53<00:00,  2.84s/files]\n",
      "Generating train split: 57264867 examples [10:20, 92244.38 examples/s] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"allenai/dolmino-mix-1124\", \"flan\", cache_dir=hf_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9939a9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of correct predictions for this example: 0.30\n",
      "Proportion of correct predictions for this example: 0.30\n",
      "Proportion of correct predictions for this example: 0.93\n",
      " entailed by the premise?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "Teacher's response: no<|endoftext|>\n",
      " entailed by the premise?\n",
      "Options:\n",
      "- yes\n",
      "- no\n",
      "- it is not possible to tell\n",
      "Teacher's response: Let's\n",
      "Proportion of correct predictions for this example: 0.18\n",
      "Proportion of correct predictions for this example: 0.83\n",
      " + $25 / month = $525 / month.<|endoftext|>\n",
      " + $25 / month = $525 / month.\n",
      "The\n",
      "Proportion of correct predictions for this example: 0.56\n",
      " 35 books are returned. On Thursday, another 15 books are withdrawn from the library. How many books are now in the library?\n",
      "There are 250 - 120 = 130 books in the library after Tuesday. There are 130 +\n",
      " 35 books are returned. On Thursday, another 15 books are withdrawn from the library. How many books are now in the library?\n",
      "On Tuesday, 120 books were taken out 250 - 120 = 130 books. On Wednesday,\n",
      "Proportion of correct predictions for this example: 0.00\n",
      "All proportions: [0.3, 0.3, 0.9285714285714286, 0.18, 0.8333333333333334, 0.56, 0.0]\n"
     ]
    }
   ],
   "source": [
    "proportions = []\n",
    "prefix_len = 200\n",
    "generation_len = 100\n",
    "n_examples = 100\n",
    "for paragraph in ds['train']['text'][:n_examples]:\n",
    "    tokenized_paragraph = tokenizer(paragraph, return_tensors='pt', return_token_type_ids=False)\n",
    "    n_paragraph_tokens = tokenized_paragraph['input_ids'].shape[1]\n",
    "    if n_paragraph_tokens < prefix_len+generation_len:\n",
    "        continue\n",
    "    paragraph_prefix = {\n",
    "        'input_ids': tokenized_paragraph['input_ids'][:, :prefix_len],\n",
    "        'attention_mask': tokenized_paragraph['attention_mask'][:, :prefix_len]}\n",
    "\n",
    "    # Generate the next GENERATION_LEN tokens using greedy sampling\n",
    "    response = olmo.generate(**paragraph_prefix, max_new_tokens=generation_len, do_sample=False)\n",
    "    n_tokens_actually_generated = response.shape[1] - prefix_len\n",
    "\n",
    "    # Decode the generated tokens and the actual tokens\n",
    "    generated_tokens = tokenizer.convert_ids_to_tokens(response[0, prefix_len:])\n",
    "    actual_tokens = tokenizer.convert_ids_to_tokens(\n",
    "        tokenized_paragraph['input_ids'][0, prefix_len:prefix_len+n_tokens_actually_generated])\n",
    "\n",
    "    # Find the longest matching subsequence on a token-by-token level\n",
    "    matcher = SequenceMatcher(None, generated_tokens, actual_tokens)\n",
    "    match = matcher.find_longest_match(0, len(generated_tokens), 0, len(actual_tokens))\n",
    "    correct_predictions = match.size\n",
    "    total_predictions = len(actual_tokens)\n",
    "\n",
    "    # Calculate the proportion of correct predictions for this example\n",
    "    proportion = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    proportions.append(proportion)\n",
    "    print(f\"Proportion of correct predictions for this example: {proportion:.2f}\")\n",
    "    if proportion >= 0.5:\n",
    "        decoded_generated = tokenizer.decode(response[0, prefix_len:])\n",
    "        decoded_actual = tokenizer.decode(\n",
    "            tokenized_paragraph['input_ids'][0, prefix_len:prefix_len+n_tokens_actually_generated])\n",
    "        print(decoded_generated)\n",
    "        print(decoded_actual)\n",
    "\n",
    "# Optionally, print all proportions\n",
    "print(\"All proportions:\", proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f96dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axolotl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
